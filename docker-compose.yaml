services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_n8n_network
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_models:/root/.ollama # Optional: Persist models
    # For GPU support (NVIDIA):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
    restart: always

  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=tie-running-sawfish-veneer
      - GENERIC_TIMEZONE=America/New_York
      - N8N_LOG_LEVEL=info
      - N8N DEFAULT_BINARY_DATA_MODE=filesystem
      - N8N_METRTICS=true
      - N8N_EXECUTIONS_DATA_PRUNE=true
      - N8N_EXECUTIONS_DATA_MAX_AGE=168
      - N8N_SECURE_COOKIE=false
    volumes:
      - n8n_data:/home/node/.n8n
      - ./local-files:/home/node/local-files
    networks:
      - n8n_network
    depends_on:
      - ollama

volumes:
  n8n_data:

networks:
  n8n_network:
    driver: bridge